{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82369c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c66cc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom imports\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join('..', '..', 'utils')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ad9189c",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [\n",
    "    'blue', 'green', 'red', 'cyan', 'magenta',\n",
    "    'yellow', 'black', 'orange', 'purple', 'brown',\n",
    "    'pink', 'grey', 'maroon', 'gold', 'chocolate',\n",
    "    'aqua', 'darkviolet', 'crimson', 'navy', 'darkgreen',\n",
    "    'peru', 'tan', 'seagreen', 'darkslategrey', 'teal'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3005409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "A = 3e-4\n",
    "use_precise_transform = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43bed90",
   "metadata": {},
   "source": [
    "## Let's load the data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24b79508",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join('..', '..', '..', 'data')\n",
    "\n",
    "root_dirs = {\n",
    "    'ideal':\n",
    "        os.path.join(data_dir, 'pdg13-n25-0.5to10GeV-0.5eta'),\n",
    "    'mat':\n",
    "        os.path.join(data_dir, 'pdg13-n25-0.5to10GeV-0.5eta-with-material-effects'),\n",
    "    'odd-bfield':\n",
    "        os.path.join(data_dir, 'pdg13-n25-0.5to10GeV-0.5eta-non-homogenous-magnetic-field'),\n",
    "    'mat-odd-bfield':\n",
    "        os.path.join(data_dir, 'pdg13-n25-0.5to10GeV-0.5eta-with-material-effects-non-homogenous-magnetic-field')\n",
    "}\n",
    "\n",
    "\n",
    "# read the hits files and dataset\n",
    "hit_files = {\n",
    "    _type: sorted([file for file in os.listdir(root_dir) if file.endswith(\"-hits.csv\")])\n",
    "    for _type, root_dir in root_dirs.items()\n",
    "}\n",
    "hit_dfs = {\n",
    "    _type: [pd.read_csv(os.path.join(root_dirs[_type], file), dtype={'particle_id':str, 'geometry_id': str})\n",
    "            for file in files]\n",
    "    for _type, files in hit_files.items()\n",
    "}\n",
    "\n",
    "# read the initial files and dataset\n",
    "initial_files = {\n",
    "    _type: sorted([file for file in os.listdir(root_dir) if file.endswith(\"-particles_initial.csv\")])\n",
    "    for _type, root_dir in root_dirs.items()\n",
    "}\n",
    "initial_dfs = {\n",
    "    _type: [pd.read_csv(os.path.join(root_dirs[_type], file), dtype={'particle_id':str, 'geometry_id': str})\n",
    "            for file in files]\n",
    "    for _type, files in initial_files.items()\n",
    "}\n",
    "\n",
    "# read the final files and dataset\n",
    "final_files = {\n",
    "    _type: sorted([file for file in os.listdir(root_dir) if file.endswith(\"-particles_final.csv\")])\n",
    "    for _type, root_dir in root_dirs.items()\n",
    "}\n",
    "final_dfs = {\n",
    "    _type: [pd.read_csv(os.path.join(root_dirs[_type], file), dtype={'particle_id':str, 'geometry_id': str})\n",
    "            for file in files]\n",
    "    for _type, files in final_files.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7854c1",
   "metadata": {},
   "source": [
    "## Let's pick an event at randrom for every case and plot the x-y and r-x views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5712821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_events = 100\n",
    "# random.seed(682021)\n",
    "\n",
    "# # pick an event randomly for each case\n",
    "# random_event = random.choice(range(0, num_events))\n",
    "# ideal_df = hit_dfs['ideal'][random_event]\n",
    "# ideal_df['r'] = np.sqrt(np.square(ideal_df['tx']) + np.square(ideal_df['ty']))\n",
    "# ideal_df['phi'] = np.arctan2(ideal_df['ty'], ideal_df['tx'])\n",
    "# ideal_df['xy_track'] = ideal_df[['r','phi']].apply(lambda pair: (pair[0], pair[1]), 1)\n",
    "# ideal_df['rz_track'] = ideal_df[['r','tz']].apply(lambda pair: (-pair[0], pair[1]), 1)\n",
    "# print(f\"Event chosen for the ideal simulation:\\t\\t\\t\\t\\t\\t{hit_files['ideal'][random_event]}\")\n",
    "\n",
    "# random_event = random.choice(range(0, num_events))\n",
    "# mat_df = hit_dfs['mat'][random_event]\n",
    "# mat_df['r'] = np.sqrt(np.square(mat_df['tx']) + np.square(mat_df['ty']))\n",
    "# mat_df['phi'] = np.arctan2(mat_df['ty'], mat_df['tx'])\n",
    "# mat_df['xy_track'] = mat_df[['r','phi']].apply(lambda pair: (pair[0], pair[1]), 1)\n",
    "# mat_df['rz_track'] = mat_df[['r','tz']].apply(lambda pair: (-pair[0], pair[1]), 1)\n",
    "# print(f\"Event chosen for the simulation with material effect:\\t\\t\\t\\t{hit_files['mat'][random_event]}\")\n",
    "\n",
    "# random_event = random.choice(range(0, num_events))\n",
    "# odd_bfield_df = hit_dfs['odd-bfield'][random_event]\n",
    "# odd_bfield_df['r'] = np.sqrt(np.square(odd_bfield_df['tx']) + np.square(odd_bfield_df['ty']))\n",
    "# odd_bfield_df['phi'] = np.arctan2(odd_bfield_df['ty'], odd_bfield_df['tx'])\n",
    "# odd_bfield_df['xy_track'] = odd_bfield_df[['r','phi']].apply(lambda pair: (pair[0], pair[1]), 1)\n",
    "# odd_bfield_df['rz_track'] = odd_bfield_df[['r','tz']].apply(lambda pair: (-pair[0], pair[1]), 1)\n",
    "# print(f\"Event chosen for the simulation with non-homogenous B:\\t\\t\\t\\t\"\n",
    "#       f\"{hit_files['odd-bfield'][random_event]}\")\n",
    "\n",
    "# random_event = random.choice(range(0, num_events))\n",
    "# mat_odd_bfield_df = hit_dfs['mat-odd-bfield'][random_event]\n",
    "# mat_odd_bfield_df['r'] = np.sqrt(np.square(mat_odd_bfield_df['tx']) + np.square(mat_odd_bfield_df['ty']))\n",
    "# mat_odd_bfield_df['phi'] = np.arctan2(mat_odd_bfield_df['ty'], mat_odd_bfield_df['tx'])\n",
    "# mat_odd_bfield_df['xy_track'] = mat_odd_bfield_df[['r','phi']].apply(lambda pair: (pair[0], pair[1]), 1)\n",
    "# mat_odd_bfield_df['rz_track'] = mat_odd_bfield_df[['r','tz']].apply(lambda pair: (-pair[0], pair[1]), 1)\n",
    "# print(f\"Event chosen for the simulation with material effect and non-homogenous B:\\t\"\n",
    "#       f\"{hit_files['mat-odd-bfield'][random_event]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0aaeee2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# from notebook1_utils import plot_views\n",
    "\n",
    "# dfs = [ideal_df, mat_df, odd_bfield_df, mat_odd_bfield_df]\n",
    "# types = ['\"ideal\"',\n",
    "#          '\"with-material-effects\"',\n",
    "#          '\\n\"non-homogenous-magnetic-field\"',\n",
    "#          '\\n\"with-material-effects-and-non-homogenous-magnetic-field\"'\n",
    "#         ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debca992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_views(dfs, types, 'tx', 'ty', 'x', 'y', colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d944929f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_views(dfs, types, 'tz', 'r', 'z', 'r', colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3458d9b",
   "metadata": {},
   "source": [
    "The distortion from the material effect and the non-homogenous magnetic field is not all that visible in these plots (at least to my eye). Let's take a look at the Hough Space also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6614edc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from notebook1_utils import plot_xy_hough\n",
    "\n",
    "# xrange = (-np.pi, np.pi)\n",
    "# ylims = (-50, 50)\n",
    "# plot_xy_hough(dfs, A, types, xrange, ylims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75a0093",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# from notebook1_utils import plot_rz_hough\n",
    "\n",
    "# xrange = (-1.5, 1.5)\n",
    "# ylims = (-25, 25)\n",
    "# plot_rz_hough(dfs, types, xrange, ylims)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5572a4be",
   "metadata": {},
   "source": [
    "The `x_range` and `ylims` parameters above can be configured in order to zoom in on any part of the plots, in order to inspect it further. Now, we will apply the Hough Transforms over the whole datasets to see how the introduction of material and non-homogenous magnetic field will assess the performance of the algorothm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba347b43",
   "metadata": {},
   "source": [
    "# Let's apply the Hough Transform to the whole datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be0f7721",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'efficiency': {'xy': {'ideal': 0,\n",
       "   'mat': 0,\n",
       "   'odd-bfield': 0,\n",
       "   'mat-odd-bfield': 0},\n",
       "  'rz': {'ideal': 0, 'mat': 0, 'odd-bfield': 0, 'mat-odd-bfield': 0},\n",
       "  'xy-rz-combo': {'ideal': 0, 'mat': 0, 'odd-bfield': 0, 'mat-odd-bfield': 0},\n",
       "  'rz-xy-combo': {'ideal': 0, 'mat': 0, 'odd-bfield': 0, 'mat-odd-bfield': 0}},\n",
       " 'fake': {'xy': {'ideal': 0, 'mat': 0, 'odd-bfield': 0, 'mat-odd-bfield': 0},\n",
       "  'rz': {'ideal': 0, 'mat': 0, 'odd-bfield': 0, 'mat-odd-bfield': 0},\n",
       "  'xy-rz-combo': {'ideal': 0, 'mat': 0, 'odd-bfield': 0, 'mat-odd-bfield': 0},\n",
       "  'rz-xy-combo': {'ideal': 0, 'mat': 0, 'odd-bfield': 0, 'mat-odd-bfield': 0}},\n",
       " 'duplicate': {'xy': {'ideal': 0,\n",
       "   'mat': 0,\n",
       "   'odd-bfield': 0,\n",
       "   'mat-odd-bfield': 0},\n",
       "  'rz': {'ideal': 0, 'mat': 0, 'odd-bfield': 0, 'mat-odd-bfield': 0},\n",
       "  'xy-rz-combo': {'ideal': 0, 'mat': 0, 'odd-bfield': 0, 'mat-odd-bfield': 0},\n",
       "  'rz-xy-combo': {'ideal': 0, 'mat': 0, 'odd-bfield': 0, 'mat-odd-bfield': 0}}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transforms = ['xy', 'rz', 'xy-rz-combo', 'rz-xy-combo']\n",
    "evaluation_metrics = ['efficiency', 'fake', 'duplicate']\n",
    "simulations = list(root_dirs.keys())\n",
    "\n",
    "stats = {\n",
    "    metric: {transform: {_type: 0 for _type in simulations} for transform in transforms}\n",
    "    for metric in evaluation_metrics\n",
    "}\n",
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39b2da06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the hyperparameters of the transforms\n",
    "xy_hyperparams = {\n",
    "    'bin-size': (0.005, 0.1),\n",
    "    'xrange': (-np.pi, np.pi),\n",
    "    'yrange': (-1000, 1000),\n",
    "    'minimum-hits-per-bin': 10,\n",
    "    'A': A,\n",
    "    'use-precise-transform': False\n",
    "}\n",
    "\n",
    "rz_hyperparams = {\n",
    "    'bin-size': (0.001, 0.05),\n",
    "    'xrange': (-2, 2),\n",
    "    'yrange': (-5000, 5000),\n",
    "    'minimum-hits-per-bin': 12\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c835a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebook1_utils import run_pipeline_over_whole_datasets\n",
    "\n",
    "run_pipeline_over_whole_datasets(hit_dfs, stats, xy_hyperparams, rz_hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31cc4d56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185473e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7e3850b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "rz_hyperparams = {\n",
    "    'bin-size': (0.001, 0.01),\n",
    "    'xrange': (-1, 1),\n",
    "    'yrange': (-2000, 2000),\n",
    "    'minimum-hits-per-bin': 10\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8e10b5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from notebook1_utils import Hough2d_pipeline, compute_qpt, compute_b\n",
    "# from metrics import efficiency_rate, fake_rate, duplicate_rate\n",
    "\n",
    "# sum_efficiency_rates = {'ideal': 0}\n",
    "# sum_fake_rates = {'ideal': 0}\n",
    "# sum_duplicate_rates = {'ideal': 0}\n",
    "\n",
    "# ideal_dfs = hit_dfs['ideal']\n",
    "# for df in tqdm(ideal_dfs):\n",
    "\n",
    "#     df['weight'] = 1.0\n",
    "#     df['r'] = np.sqrt(np.square(df['tx']) + np.square(df['ty']))\n",
    "#     df['phi'] = np.arctan2(df['ty'], df['tx'])\n",
    "#     df['xy_track'] = df[['r','phi']].apply(lambda pair: (pair[0], pair[1]), 1)\n",
    "#     df['rz_track'] = df[['r','tz']].apply(lambda pair: (-pair[0], pair[1]), 1)\n",
    "    \n",
    "#     tracks = list(df['rz_track'])\n",
    "#     _, rz_est = Hough2d_pipeline(tracks, rz_hyperparams, compute_b)\n",
    "        \n",
    "#     df['track'] = df['rz_track']\n",
    "#     sum_efficiency_rates['ideal'] += efficiency_rate(rz_est.values(), df)\n",
    "#     sum_fake_rates['ideal'] += fake_rate(rz_est.values(), df)\n",
    "#     sum_duplicate_rates['ideal'] += duplicate_rate(rz_est.values(), df)\n",
    "    \n",
    "# sum_efficiency_rates = {key: val / 100 for key, val in sum_efficiency_rates.items()}\n",
    "# sum_efficiency_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c1705f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum_duplicate_rates = {key: val / 100 for key, val in sum_duplicate_rates.items()}\n",
    "# sum_duplicate_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8afa7c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum_fake_rates = {key: val / 100 for key, val in sum_fake_rates.items()}\n",
    "# sum_fake_rates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761c0848",
   "metadata": {},
   "source": [
    "### First we will run each transformation to see how it performs on its own and then we will compare them with their combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19602b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # first define the hyperparameters of the transforms\n",
    "# xy_hyperparams = {\n",
    "#     'bin-size': (0.0025, 0.05),\n",
    "#     'phi-range': (0, np.pi),\n",
    "#     'qpt-range': (-200, 200),\n",
    "#     'minimum-hits-per-bin': 10,\n",
    "#     'use-sin': True\n",
    "# }\n",
    "\n",
    "# rz_hyperparams = {\n",
    "#     'bin-size': (0.005, 0.1),\n",
    "#     'r-range': (-1, 1),\n",
    "#     'z-range': (-1000, 1000),\n",
    "#     'minimum-hits-per-bin': 12\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a273a7f9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# from notebook2_utils import xy_pipeline, compute_all_intersections, rz_pipeline\n",
    "\n",
    "# _, xy_est_tracks_to_hits = xy_pipeline(all_xy_tracks, xy_hyperparams)\n",
    "\n",
    "# rz_intersections = compute_all_intersections(all_rz_tracks)\n",
    "# rz_est_tracks_to_hits = rz_pipeline(all_rz_tracks, rz_intersections, rz_hyperparams)\n",
    "\n",
    "# len(xy_est_tracks_to_hits), len(rz_est_tracks_to_hits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f102d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from metrics import efficiency_rate, fake_rate, duplicate_rate\n",
    "\n",
    "# df['track'] = df['xy_track']\n",
    "# xy_eff = efficiency_rate(xy_est_tracks_to_hits.values(), df)\n",
    "# xy_fak = fake_rate(xy_est_tracks_to_hits.values(), df)\n",
    "# xy_dup = duplicate_rate(xy_est_tracks_to_hits.values(), df)\n",
    "\n",
    "# df['track'] = df['rz_track']\n",
    "# rz_eff = efficiency_rate(rz_est_tracks_to_hits.values(), df)\n",
    "# rz_fak = fake_rate(rz_est_tracks_to_hits.values(), df)\n",
    "# rz_dup = duplicate_rate(rz_est_tracks_to_hits.values(), df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9422a65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
